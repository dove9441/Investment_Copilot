from langchain_community.document_loaders import WebBaseLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
import dotenv
import os
from langchain_groq import ChatGroq
from langchain.docstore.document import Document # í…ìŠ¤íŠ¸ë¥¼ document ê°ì²´ë¡œ ë³€í™˜
import time
import queue as q
import threading
import json
from fastapi import Request, FastAPI, Response
from langchain.chains import LLMChain
dotenv.load_dotenv()

##### ê¸°ëŠ¥ í•¨ìˆ˜ êµ¬í˜„ ë‹¨ê³„ #####
# ë©”ì‹œì§€ ì „ì†¡
def textResponseFormat(bot_response):
    #print('------------BOT_RESONSE-------------')
    #print(bot_response)
    response = {"version":"2.0", "template":{"outputs":[{"simpleText":{"text":bot_response}}], "quickReplies":[]}}
    return response

# ì‚¬ì§„ ì „ì†¡
def imageResponseFormat(bot_response, prompt):
    output_text = prompt + "ë‚´ìš©ì— ê´€í•œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤"
    response = {"version":"2.0", "template":{"outputs":[{"simpleImage":{"imageUrl":bot_response, "altText":output_text}}], "quickReplies":[]}}
    return response

# ì‘ë‹µ ì´ˆê³¼ ì‹œ ë‹µë³€
def timeover():
    response = { "version":"2.0", "template":{
        "outputs":[
            {
                "simpleText":{
                    "text":"ì•„ì§ ì œê°€ ìƒê°ì´ ëë‚˜ì§€ ì•Šì•˜ì–´ìš” ğŸ™ğŸ™ \nì ì‹œ í›„ ì•„ë˜ ë§í’ì„ ì„ ëˆŒëŸ¬ì£¼ì„¸ìš” ğŸ‘†"
                }
            }
        ],
        "quickReplies":[
            {
                "action":"message",
                "label":"ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”? ğŸ™‹â€â™‚ï¸",
                "messageText":"ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”?"
            }
        ]
    }}
    return response




# í…ìŠ¤íŠ¸ íŒŒì¼ ì´ˆê¸°í™”
def dbReset(filename):
    with open(filename, 'w') as f:
        f.write("")


##### ì„œë²„ ìƒì„± ë‹¨ê³„ #####

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "kakaoTest"}

@app.post("/chat/")
async def chat(request: Request):
    kakaorequest = await request.json()
    return mainChat(kakaorequest)

#### ë©”ì¸ í•¨ìˆ˜ êµ¬í˜„ ë‹¨ê³„ ######

# ë©”ì¸ í•¨ìˆ˜
def mainChat(kakaorequest):
    #print(json.dumps(kakaorequest, indent=2))
    run_flag = False
    start_time = time.time()
    #kakaorequest = json.loads(event['body'])
    # ì‘ë‹µ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    cwd = os.getcwd()
    filename = cwd + "./botlog.txt"
    if not os.path.exists(filename):
        with open(filename, "w") as f:
            f.write("")
    else:
        print("File Exists")

    # ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì‹¤í–‰
    response_queue = q.Queue()
    request_respond = threading.Thread(target=responseOpenAI, args = (kakaorequest, response_queue, filename))
    request_respond.start()

    # ë‹µë³€ ìƒì„± ì‹œê°„ ì²´í¬
    while (time.time() - start_time < 3.5):
        if not response_queue.empty():
            # 3.5ì´ˆ ì•ˆì— ë‹µë³€ì´ ì™„ì„±ë˜ë©´ ë°”ë¡œ ê°’ì„ ë°˜í™˜
            response = response_queue.get()
            run_flag = True
            break
        # ì•ˆì •ì ì¸ êµ¬ë™ì„ ìœ„í•œ ë”œë ˆì´ íƒ€ì„ ì„¤ì •
        time.sleep(0.01)
    # ì‘ë‹µì´ 3.5ì´ˆ ì´ë‚´ì— ì˜¤ì§€ ì•Šìœ¼ë©´
    if run_flag == False:
        # ìƒê° ëë‚¨ë²„íŠ¼ ì¶œë ¥í•¨ìˆ˜ í˜¸ì¶œ
        response = timeover()
    return Response(content=json.dumps(response), media_type='application/json')

# ë‹µë³€/ì‚¬ì§„ ìš”ì²­ ë° ì‘ë‹µ í™•ì¸ í•¨ìˆ˜
def responseOpenAI(request, response_queue, filename):
    #print(json.dumps(request, indent=2))
    # ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë‹µë³€ ì™„ì„± ì—¬ë¶€ë¥¼ ë‹¤ì‹œ ë´¤ì„ ì‹œ
    if 'ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”?' in request["userRequest"]["utterance"]:
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì—´ê¸°
        with open(filename) as f:
            last_update = f.read()
        # í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ ì €ì¥ëœ ì •ë³´ê°€ ìˆì„ ê²½ìš°
        if len(last_update.split()) > 1:
            kind, bot_res, prompt = last_update.split()[0], last_update.split()[1], last_update.split()[2]
            if kind == "img":
                response_queue.put(imageResponseFormat(bot_res, prompt))
            else:
                response_queue.put(textResponseFormat(bot_res))
            dbReset(filename)

    # LLM ë‹µë³€ì„ ìš”ì²­í•œ ê²½ìš°
    elif '/ask' in request["userRequest"]["utterance"]: 
        dbReset(filename)
        prompt = request["userRequest"]["utterance"].replace("/ask", "")
        bot_res = getRagResponse_LLAMA(prompt)#getTextFromLLAMA(prompt)
        response_queue.put(textResponseFormat(bot_res))

        save_log = "ask" + " " + str(bot_res) + " " + str(prompt)
        with open(filename, 'w') as f:
            f.write(save_log)

    # ì•„ë¬´ ë‹µë³€ ìš”ì²­ì´ ì—†ëŠ” ì±„íŒ…ì¼ ê²½ìš°
    else:
        # ê¸°ë³¸ response ê°’
        defaultText = "ì•„ë¬´ ë‹µë³€ ìš”ì²­ì´ ì—†ëŠ” ë©”ì‹œì§€ì…ë‹ˆë‹¤."
        base_response = {
                            "version": "2.0",
                            "template": {
                                "outputs": [
                                {
                                    "simpleText": {
                                        "text": "default"
                                    }
                                }
                                ]
                            }
                            }
        response_queue.put(textResponseFormat(defaultText))




def getTextFromLLAMA(prompt):
    llm = ChatGroq(model="llama-3.1-8b-instant")#llama-3.1-70b-versatile")
    combine_prompt = PromptTemplate(input_variables=['text'], template="You are an participatnt in 1:1 dialogue. Response about quesition. : {text}.")
    chain = LLMChain(llm=llm, prompt=combine_prompt, verbose=True)
    response = chain.invoke({'text':prompt})
    return response['text']



def getSummeryFromLLM(text):
    # ê° Chunk ë‹¨ìœ„ì˜ í…œí”Œë¦¿
    template = '''ë‹¤ìŒì˜ ë‚´ìš©ì„ í•œê¸€ë¡œ ìš”ì•½í•´ì¤˜:
    {text}
    '''
    # ì „ì²´ ë¬¸ì„œ(í˜¹ì€ ì „ì²´ Chunk)ì— ëŒ€í•œ ì§€ì‹œ(instruct) ì •ì˜
    combine_template = '''{text}

    ìš”ì•½ì˜ ê²°ê³¼ëŠ” ë‹¤ìŒì˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
    ì œëª©: ê¸°ì‚¬ ì œëª©
    ìš”ì•½: 3ì¤„ë¡œ ìš”ì•½ëœ ë‚´ìš©
    ì„¸ë¶€ë‚´ìš©: ì£¼ìš”ë‚´ìš©ì„ ì‘ì„±
    '''
    # í…œí”Œë¦¿ ìƒì„±
    prompt = PromptTemplate(template=template, input_variables=['text'])
    combine_prompt = PromptTemplate(template=combine_template, input_variables=['text'])

    # LLM ê°ì²´ ìƒì„±
    llm = ChatGroq(model="llama-3.1-8b-instant")#llama-3.1-70b-versatile")
    #llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-16k')

    # ìš”ì•½ì„ ë„ì™€ì£¼ëŠ” load_summarize_chain
    chain = load_summarize_chain(llm,  map_prompt=prompt, combine_prompt=combine_prompt, chain_type="map_reduce", verbose=True)
    text_docs = Document(page_content=text)
    summerized_msg = chain.invoke([text_docs])
    return summerized_msg


################################# WEBSEARCH RAG TEST ######################
import streamlit as st
from langchain_community.document_loaders import AsyncChromiumLoader
from langchain_community.document_transformers import BeautifulSoupTransformer
from duckduckgo_search import DDGS
import re

# performs DuckDuckGo search, urls are extracted and status checked
# 
def ddg_search(query):
    results = DDGS().text(query, max_results=5)
    urls = []
    for result in results:
        url = result['href']
        urls.append(url)

    docs = get_page(urls)

    content = []
    for doc in docs:
        page_text = re.sub("\n\n+", "\n", doc.page_content)
        text = truncate(page_text)
        content.append(text)
    return content

# retrieves pages and extracts text by tag
def get_page(urls):
    loader = AsyncChromiumLoader(urls)
    html = loader.load()

    bs_transformer = BeautifulSoupTransformer()
    docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=["p"], remove_unwanted_tags=["a"])

    return docs_transformed

# helper function to reduce the amount of text
def truncate(text):
    words = text.split()
    truncated = " ".join(words[:400])

    return truncated

def getRagResponse_LLAMA(query):
    search_results = ddg_search(query)
    if not search_results:
        return "No search results found or an error occurred."

    # LLM initialization
    llm = ChatGroq(model="llama-3.1-8b-instant")  # Ensure this model exists and is configured

    # Prompt template
    combine_prompt = PromptTemplate(
        input_variables=['text', 'search_results'],
        template="You are a participant in a 1:1 dialogue. Respond to the question using the search results. "
                 "Question: {text}\n"
                 "Search Results: {search_results}\n"
                 "Answer:"
    )

    # Chain setup
    chain = LLMChain(llm=llm, prompt=combine_prompt, verbose=True)
    try:
        response = chain.invoke({'text': query, 'search_results': "\n".join(search_results)})
        return response['text']
    except Exception as e:
        print(f"Error during LLM invocation: {e}")
        return "An error occurred while generating the response."




sampletext = """
GOP senators could pay a price for a prolonged confirmation fight
If Trump sticks with his pick, Republican senators feeling the MAGA movementâ€™s pressure could be forced to defend Gaetz for weeks. That could land them in a tricky spot. Despite the threat that senators could face primaries if they break with the president-elect, votes for a compromised nominee could also haunt those seeking reelection in statewide races in 2026.

Cramer is raising logical political issues. Yet Trump is such a unique figure that the normal calculations may not apply.

Matt Gaetz speaks during a news conference at the US Capitol on February 13.
Related article
Women testified to House panel that they were paid for sexual favors by Gaetz, lawyer says

Historically, a conventional president might see the uproar surrounding his pick, assess the shifting political sands and quietly withdraw support â€” reasoning that thereâ€™s little point in damaging their precious authority before their term even starts. Such political capital might be better spent on aggressively implementing an agenda in the first 100 days than on a pick who could already be doomed. In Trumpâ€™s case, sacrificing Gaetz could also ease the way for other provocative Cabinet picks, including Fox News anchor Pete Hegseth, whom Trump wants to serve as defense secretary, and Robert F. Kennedy Jr., the vaccine skeptic he wants to run the Health and Human Services Department. Senators might be just about ready to defy the president-elect on one selection, but a wholesale rejection of his choices would be political folly for them.

But Trump is also making this about far more than Gaetz, creating a test of power that reflects his own self-confidence, the balance of power in the new Congress and his belief that the GOP Senate should be at his service and not be a moderating force.

The president-electâ€™s unorthodox pick of the Florida Republican â€“ and the muscle that heâ€™s already put into his candidacy â€“ means that Trump may soon approach the point where it will cost him more political capital to fold on Gaetz than to keep trying to get him installed â€“ whatever it takes.

Ever since Trump shocked Capitol Hill and delighted his most committed supporters by selecting an ultra loyalist who has said the FBI should be abolished if it wonâ€™t â€œcome to heel,â€ itâ€™s been clear that this pick is different. The president-elect could have chosen just about anyone in Washington, and they would have been less controversial than Gaetz.

But the Florida Republican shares the president-electâ€™s belief that the Justice Department has victimized Trump and needs to be purged."""





        